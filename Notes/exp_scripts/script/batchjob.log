nohup: ignoring input
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 0
user  0  proxy  (0, 0)
user  1  proxy  (0, 1)
userNumList ['1', '2', '3', '4', '5', '6', '7', '8']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 2
total sshimNum 0
total userNum 2

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor57.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
proxy-0 <thor58.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor49.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor59.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
2 2
assign user 0
assign user 1
total sshimNum 0
totalProxyNum: 2
total userNum 2
all dc
dc  0
coorNum 0
proxy  0 thor57.mpi-sws.org 60000
proxy  1 thor58.mpi-sws.org 60001
database  0 thor56.mpi-sws.org 50000
user  0 thor49.mpi-sws.org 60002
user  1 thor59.mpi-sws.org 60003
user  0  proxy  (0, 0)
user  1  proxy  (0, 1)
set logical clock
create connections
thor56.mpi-sws.org
thor57.mpi-sws.org
node  thor57.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor58.mpi-sws.org
node  thor58.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor49.mpi-sws.org
node  thor49.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor59.mpi-sws.org
node  thor59.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-25 23:22:34
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-25 23:22:34' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 0:23:51']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 0:23:52']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser1/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-25 23:33:54
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-25 23:33:54' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =2' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =2' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 0:35:11']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 0:35:12']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser2/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-25 23:45:18
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-25 23:45:18' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =3' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =3' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 0:46:35']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 0:46:36']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser3/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-25 23:56:17
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-25 23:56:17' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =4' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =4' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 0:57:34']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 0:57:35']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser4/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 00:07:36
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 00:07:36' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =5' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =5' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 1:8:53']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 1:8:54']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser5/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 00:18:57
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 00:18:57' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =6' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =6' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 1:20:14']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 1:20:15']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser6/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 00:30:22
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 00:30:22' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =7' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =7' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 1:31:39']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 1:31:40']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser7/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 00:41:21
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 00:41:21' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =8' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =8' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 1:42:38']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 1:42:39']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/simulateUser8/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc2proxiesLocalConfigOrig.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-0-20-rubis-1dc2proxy2user/rubis1dc2proxiesLocalConfigOrig.txt
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc2proxiesLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc2Proxies.db Run is running
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc2proxiesLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc2Proxies.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 0
user  0  proxy  (0, 0)
user  1  proxy  (0, 1)
userNumList ['1', '2', '3', '4', '5', '6', '7', '8']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 2
total sshimNum 0
total userNum 2

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor57.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
proxy-0 <thor58.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor49.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor59.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
2 2
assign user 0
assign user 1
total sshimNum 0
totalProxyNum: 2
total userNum 2
all dc
dc  0
coorNum 0
proxy  0 thor57.mpi-sws.org 60000
proxy  1 thor58.mpi-sws.org 60001
database  0 thor56.mpi-sws.org 50000
user  0 thor49.mpi-sws.org 60002
user  1 thor59.mpi-sws.org 60003
user  0  proxy  (0, 0)
user  1  proxy  (0, 1)
set logical clock
create connections
thor56.mpi-sws.org
thor57.mpi-sws.org
node  thor57.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor58.mpi-sws.org
node  thor58.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor49.mpi-sws.org
node  thor49.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor59.mpi-sws.org
node  thor59.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 00:52:44
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 00:52:44' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 1:54:1']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 1:54:2']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser1/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 01:04:04
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 01:04:04' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =2' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =2' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 2:5:20']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 2:5:21']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser2/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 01:15:28
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 01:15:28' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =3' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =3' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 2:16:46']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 2:16:47']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser3/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 01:26:29
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 01:26:29' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =4' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =4' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 2:27:46']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 2:27:47']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser4/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
R
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 01:37:47
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 01:37:47' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =5' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =5' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 2:39:3']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 2:39:5']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser5/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 01:49:07
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 01:49:07' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =6' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =6' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 2:50:23']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 2:50:25']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser6/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 02:00:32
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 02:00:32' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =7' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =7' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 3:1:48']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 3:1:49']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser7/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor58.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor49.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor58.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor49.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 02:11:30
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 02:11:30' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor58.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor58.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor58.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =8' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =8' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor58.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor58.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor49.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor58.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor49.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 3:12:47']
node  thor49.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/rubis_dcId0_userId0.tar
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 3:12:48']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/simulateUser8/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc2proxiesLocalConfigOrig.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-1-51-rubis-1dc2proxy2user/rubis1dc2proxiesLocalConfigOrig.txt
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc2proxiesLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc2Proxies.db Run is running
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc2proxiesLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc2Proxies.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
<?xml version='1.0' encoding='UTF-8'?>
<dataCenters dcNum='1'>
  <dataCenter RemotecdIP='thor48.mpi-sws.org' cdPort='60000' cdIP='thor48.mpi-sws.org' RemotecdPort='60001'>
    <storageShims ssNum='1'>
      <storageShim ssIP='thor50.mpi-sws.org' ssPort='60002'/>
    </storageShims>
    <webProxies wpNum='2'>
      <webproxy wpPort='60003' wpIP='thor51.mpi-sws.org'/>
      <webproxy wpPort='60004' wpIP='thor52.mpi-sws.org'/>
    </webProxies>
  </dataCenter>
</dataCenters>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 1
user  0  proxy  (0, 0)
user  1  proxy  (0, 1)
userNumList ['1', '2', '3', '4', '5', '6', '7', '8']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 2
total sshimNum 1
total userNum 2

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor51.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
proxy-0 <thor52.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor53.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor54.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor01.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor50.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor48.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
2 2
assign user 0
assign user 1
total sshimNum 1
totalProxyNum: 2
total userNum 2
all dc
dc  0
coorNum 1
proxy  0 thor51.mpi-sws.org 60003
proxy  1 thor52.mpi-sws.org 60004
sshim  0 thor50.mpi-sws.org 60002
database  0 thor56.mpi-sws.org 50000
user  0 thor53.mpi-sws.org 60005
user  1 thor54.mpi-sws.org 60006
user  0  proxy  (0, 0)
user  1  proxy  (0, 1)
set logical clock
we make DcConfigXml here
create connections
thor48.mpi-sws.org
node  thor48.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor50.mpi-sws.org
node  thor50.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor56.mpi-sws.org
thor51.mpi-sws.org
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor52.mpi-sws.org
node  thor52.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor53.mpi-sws.org
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor54.mpi-sws.org
node  thor54.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
R
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 02:22:45
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 02:22:45' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 3:24:4']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 3:24:5']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 02:33:58
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 02:33:58' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =2' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =2' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 3:35:16']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 3:35:17']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 02:45:15
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 02:45:15' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =3' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =3' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 3:46:33']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 3:46:34']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 02:56:13
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 02:56:13' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =4' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =4' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 3:57:22']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 3:57:23']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 03:07:20
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 03:07:20' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =5' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =5' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 4:8:28']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 4:8:29']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 03:18:24
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 03:18:24' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =6' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =6' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 4:19:33']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 4:19:34']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 03:29:32
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 03:29:32' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 1  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =7' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =7' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 4:31:12']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 4:31:13']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 03:41:08
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 03:41:08' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =8' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =8' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 4:42:17']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 4:42:18']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc2proxiesLocalConfigSifter.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-3-21-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/rubis1dc2proxiesLocalConfigSifter.txt
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc2proxiesLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc2Proxies.db Run is running
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc2proxiesLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc2Proxies.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
<?xml version='1.0' encoding='UTF-8'?>
<dataCenters dcNum='1'>
  <dataCenter RemotecdIP='thor48.mpi-sws.org' cdPort='60000' cdIP='thor48.mpi-sws.org' RemotecdPort='60001'>
    <storageShims ssNum='1'>
      <storageShim ssIP='thor50.mpi-sws.org' ssPort='60002'/>
    </storageShims>
    <webProxies wpNum='2'>
      <webproxy wpPort='60003' wpIP='thor51.mpi-sws.org'/>
      <webproxy wpPort='60004' wpIP='thor52.mpi-sws.org'/>
    </webProxies>
  </dataCenter>
</dataCenters>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 1
user  0  proxy  (0, 0)
user  1  proxy  (0, 1)
userNumList ['1', '2', '3', '4', '5', '6', '7', '8']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 2
total sshimNum 1
total userNum 2

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor51.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
proxy-0 <thor52.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor53.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor54.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor01.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor50.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor48.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
2 2
assign user 0
assign user 1
total sshimNum 1
totalProxyNum: 2
total userNum 2
all dc
dc  0
coorNum 1
proxy  0 thor51.mpi-sws.org 60003
proxy  1 thor52.mpi-sws.org 60004
sshim  0 thor50.mpi-sws.org 60002
database  0 thor56.mpi-sws.org 50000
user  0 thor53.mpi-sws.org 60005
user  1 thor54.mpi-sws.org 60006
user  0  proxy  (0, 0)
user  1  proxy  (0, 1)
set logical clock
we make DcConfigXml here
create connections
thor48.mpi-sws.org
node  thor48.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor50.mpi-sws.org
node  thor50.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor56.mpi-sws.org
thor51.mpi-sws.org
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor52.mpi-sws.org
node  thor52.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor53.mpi-sws.org
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor54.mpi-sws.org
node  thor54.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 03:52:26
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 03:52:26' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 4:53:45']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 4:53:46']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser1/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 04:03:39
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 04:03:39' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =2' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =2' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 5:4:48']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 5:4:49']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser2/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 04:14:47
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 04:14:47' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =3' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =3' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 5:16:6']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 5:16:7']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser3/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 04:25:45
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 04:25:45' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =4' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =4' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 5:27:4']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 5:27:5']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser4/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 04:36:56
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 04:36:56' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =5' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =5' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 5:38:5']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 5:38:6']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser5/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 04:47:58
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 04:47:58' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =6' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =6' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 5:49:18']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 5:49:19']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser6/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 04:59:18
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 04:59:18' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =7' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =7' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 6:0:56']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 6:0:57']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser7/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor54.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor52.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor52.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
node  thor54.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 05:10:35
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 05:10:35' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc2proxy1storage2user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc2proxy1storage2user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor52.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 1
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
node  thor52.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor52.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 1  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =8' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
try to start user  0 1
connect to  0 1
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =8' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 1 &> user0-1.log &
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 1
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor52.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor52.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
killall user  0 1
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor54.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/host-manager.2014-01-26.log.dcid0.proxyid0
get a list of files from  thor52.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/catalina.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/localhost.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/manager.2014-01-26.log.dcid0.proxyid1
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/host-manager.2014-01-26.log.dcid0.proxyid1
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 6:11:44']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/rubis_dcId0_userId0.tar
get a list of files from  thor54.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_1_2014-1-26 6:11:45']
node  thor54.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId1.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId1.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/simulateUser8/rubis_dcId0_userId1.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc2proxiesLocalConfigSifter.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-4-50-rubis-1dc2proxy1storage2user-as20-p10-c20-ss20/rubis1dc2proxiesLocalConfigSifter.txt
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc2proxiesLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc2Proxies.db Run is running
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc2proxiesLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc2Proxies.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
Buildfile: /home/chengli/newJava/src/applications/RUBiStxmud/Client/build.xml

clean:
   [delete] Deleting directory /home/chengli/newJava/src/applications/RUBiStxmud/Client/build

BUILD SUCCESSFUL
Total time: 0 seconds
Buildfile: /home/chengli/newJava/src/applications/RUBiStxmud/Client/build.xml

init:
    [mkdir] Created dir: /home/chengli/newJava/src/applications/RUBiStxmud/Client/build

compile:
    [javac] /home/chengli/newJava/src/applications/RUBiStxmud/Client/build.xml:21: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 16 source files to /home/chengli/newJava/src/applications/RUBiStxmud/Client/build
    [javac] depend attribute is not supported by the modern compiler
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.

dist:
      [jar] Building jar: /home/chengli/newJava/src/applications/RUBiStxmud/Client/rubis_client.jar

BUILD SUCCESSFUL
Total time: 2 seconds

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 0
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 0
total userNum 1

 ===> start create result folders
experiment handler initialization
You want to deploy the experiment since you haven't done yet or code has been changed!

 ===> deploy experiment
initiate datacenters
proxy-0 <thor57.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor59.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 0
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 0
proxy  0 thor57.mpi-sws.org 60000
database  0 thor56.mpi-sws.org 50000
user  0 thor59.mpi-sws.org 60001
user  0  proxy  (0, 0)
set logical clock
create connections
thor56.mpi-sws.org
thor57.mpi-sws.org
node  thor57.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor59.mpi-sws.org
node  thor59.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt

 ===> prepare local code and jars
cd /home/chengli/newJava/src/applications/RUBiStxmud/Client && ant clean && ant

 ===> deploy proxies to remote nodes

 ===> deploy website
node  thor57.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; mkdir /var/tmp/chengli/txmud

 ===> compile txmud
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava && ant clean && ant

 ===> Downloading Tomcat6
tomcat directory already exists

 ===> downloading mysql driver
mysql driver already installed
change proxy to connect to database  thor56.mpi-sws.org 50000
node  thor57.mpi-sws.org
execute cmd  sed -i '/datasource.url/c  \datasource.url    jdbc:mysql://thor56.mpi-sws.org:50000/rubis? '  /home/chengli/newJava/src/applications/RUBiStxmud/Servlets/mysql.properties

 ===> deploy users to remote nodes
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//rubis_client.jar
copy jar file  to client node 0 0
Copying file from source  /home/chengli/newJava/src/applications/RUBiStxmud/Client/rubis_client.jar  to destination /var/tmp/chengli//rubis_client.jar
copy client properties to client node 0 0
Copying file from source  /home/chengli/newJava/src/applications/RUBiStxmud/Client/rubis.properties  to destination /var/tmp/chengli//rubis.properties
node  thor59.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli//bench

 ===> experiment is deployed!
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Deploy is running
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Deploy finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 0
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 0
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed your code, now you want to config all of them

 ===> configure experiment
initiate datacenters
proxy-0 <thor57.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor59.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 0
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 0
proxy  0 thor57.mpi-sws.org 60000
database  0 thor56.mpi-sws.org 50000
user  0 thor59.mpi-sws.org 60001
user  0  proxy  (0, 0)
set logical clock
create connections
thor56.mpi-sws.org
thor57.mpi-sws.org
node  thor57.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor59.mpi-sws.org
node  thor59.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
install proxy all websites
install website
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant clean undeploy dist deploy -Dbackend=mysql -Dtotalproxy=1 -DdcCount=1 -DdcId=0 -DproxyId=0 -Ddbpool=100
install all user websites
try to configure user  0 0
connect to  0 0
node  thor59.mpi-sws.org
execute cmd  sed -i '/workload_up_ramp_time_in_ms/c  \workload_up_ramp_time_in_ms  =60000' /var/tmp/chengli//rubis.properties && sed -i '/workload_session_run_time_in_ms/c  \workload_session_run_time_in_ms  =300000' /var/tmp/chengli//rubis.properties && sed -i '/workload_down_ramp_time_in_ms/c  \workload_down_ramp_time_in_ms  =60000' /var/tmp/chengli//rubis.properties && sed -i '/httpd_hostname/c \httpd_hostname =thor57.mpi-sws.org' /var/tmp/chengli//rubis.properties
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Config is running
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Config finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 0
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 0
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor57.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor59.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 0
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 0
proxy  0 thor57.mpi-sws.org 60000
database  0 thor56.mpi-sws.org 50000
user  0 thor59.mpi-sws.org 60001
user  0  proxy  (0, 0)
set logical clock
create connections
thor56.mpi-sws.org
thor57.mpi-sws.org
node  thor57.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor59.mpi-sws.org
node  thor59.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user/simulateUser1

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 05:21:58
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 05:21:58' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
copy user file
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 6:23:13']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user/simulateUser1/rubis_dcId0_userId0.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc1proxyLocalConfigOrig.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-20-rubis-1dc1proxy1user/rubis1dc1proxyLocalConfigOrig.txt
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Run is running
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 0
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 0
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor57.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor59.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 0
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 0
proxy  0 thor57.mpi-sws.org 60000
database  0 thor56.mpi-sws.org 50000
user  0 thor59.mpi-sws.org 60001
user  0  proxy  (0, 0)
set logical clock
create connections
thor56.mpi-sws.org
thor57.mpi-sws.org
node  thor57.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor59.mpi-sws.org
node  thor59.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user/simulateUser1

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 05:33:18
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 05:33:18' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
copy user file
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 6:34:33']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user/simulateUser1/rubis_dcId0_userId0.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc1proxyLocalConfigOrig.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-31-rubis-1dc1proxy1user/rubis1dc1proxyLocalConfigOrig.txt
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Run is running
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 0
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 0
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor57.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor59.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor100.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 0
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 0
proxy  0 thor57.mpi-sws.org 60000
database  0 thor56.mpi-sws.org 50000
user  0 thor59.mpi-sws.org 60001
user  0  proxy  (0, 0)
set logical clock
create connections
thor56.mpi-sws.org
thor57.mpi-sws.org
node  thor57.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor59.mpi-sws.org
node  thor59.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user/simulateUser1

 ===>run test now

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor57.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor59.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor57.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor59.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 05:44:43
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 05:44:43' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
try to start all proxies
node  thor57.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor57.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor57.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor57.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor57.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor59.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy webproxy file
get a list of files from  thor57.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
copy user file
get a list of files from  thor59.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 6:45:59']
node  thor59.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user/simulateUser1/rubis_dcId0_userId0.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc1proxyLocalConfigOrig.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original/2014-1-26-6-42-rubis-1dc1proxy1user/rubis1dc1proxyLocalConfigOrig.txt
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Run is running
python rubisOrigExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/Original chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigOrig.txt notused  nondebug Local nodesRUBiSOrig1dc1Proxy.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
<?xml version='1.0' encoding='UTF-8'?>
<dataCenters dcNum='1'>
  <dataCenter RemotecdIP='thor48.mpi-sws.org' cdPort='60000' cdIP='thor48.mpi-sws.org' RemotecdPort='60001'>
    <storageShims ssNum='1'>
      <storageShim ssIP='thor50.mpi-sws.org' ssPort='60002'/>
    </storageShims>
    <webProxies wpNum='1'>
      <webproxy wpPort='60003' wpIP='thor51.mpi-sws.org'/>
    </webProxies>
  </dataCenter>
</dataCenters>
Buildfile: /var/tmp/workspace/georeplication/build.xml

clean:
   [delete] Deleting directory /var/tmp/workspace/georeplication/bin/classes
   [delete] Deleting directory /var/tmp/workspace/georeplication/bin/lib
   [delete] Deleting directory /var/tmp/workspace/georeplication/bin
   [delete] Deleting directory /var/tmp/workspace/georeplication/docs
   [delete] Deleting directory /var/tmp/workspace/georeplication/dist

BUILD SUCCESSFUL
Total time: 0 seconds
Buildfile: /var/tmp/workspace/georeplication/build.xml

clean:

makedir:
    [mkdir] Created dir: /var/tmp/workspace/georeplication/bin
    [mkdir] Created dir: /var/tmp/workspace/georeplication/bin/classes
    [mkdir] Created dir: /var/tmp/workspace/georeplication/bin/lib
    [mkdir] Created dir: /var/tmp/workspace/georeplication/docs
    [mkdir] Created dir: /var/tmp/workspace/georeplication/dist
    [mkdir] Created dir: /var/tmp/workspace/georeplication/dist/exec

compile:
    [javac] Compiling 347 source files to /var/tmp/workspace/georeplication/bin/classes
    [javac] /var/tmp/workspace/georeplication/src/replicationlayer/core/txstore/scratchpad/rdbms/jdbc/TxMudConnection.java:20: warning: CachedRowSetImpl is internal proprietary API and may be removed in a future release
    [javac] import com.sun.rowset.CachedRowSetImpl;
    [javac]                      ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 warning

jar:
      [jar] Building jar: /var/tmp/workspace/georeplication/bin/lib/georeplication.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/javafileparsertest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/projectparsertest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/sqlmaterializertest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/projectparsertpcw.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/projectparserrubis.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/templatecreatortpcw.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/templatecreatorrubis.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/dbclasscreatortpcw.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/dbclasscreatorrubis.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/controlflowgraphtest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/shdoptest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/simpleexpeval.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/wpchecker.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/pathanalyzertest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/dbclasscreatortest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/templatecreatortest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/annotationparsertest.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/coordinator.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/storageshim.jar
      [jar] Building jar: /var/tmp/workspace/georeplication/dist/preloadDB.jar

onejar:
  [one-jar] main.jar fs=georeplication.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/bin/lib/georeplication-all.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=javafileparsertest.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/javafileparsertest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=projectparsertest.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/projectparsertest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/sqlmaterializertest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=projectparsertpcw.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/projectparsertpcw-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=projectparserrubis.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/projectparserrubis-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=templatecreatortpcw.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/templatecreatortpcw-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=templatecreatorrubis.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/templatecreatorrubis-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=dbclasscreatortpcw.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/dbclasscreatortpcw-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=dbclasscreatorrubis.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/dbclasscreatorrubis-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=controlflowgraphtest.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/controlflowgraphtest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=shdoptest.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/shdoptest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=simpleexpeval.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/simpleexpeval-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=wpchecker.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/wpchecker-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=pathanalyzertest.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/pathanalyzertest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=dbclasscreatortest.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/dbclasscreatortest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=templatecreatortest.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/templatecreatortest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=annotationparsertest.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/annotationparsertest-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=coordinator.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/coordinator-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=storageshim.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/storageshim-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=preloadDB.jar
  [one-jar] Building jar: /var/tmp/workspace/georeplication/dist/exec/preloadDB-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.

main:

BUILD SUCCESSFUL
Total time: 13 seconds
Buildfile: /home/chengli/newJava/build.xml

clean:
   [delete] Deleting directory /var/tmp/chengli/output/txmud/build
   [delete] Deleting directory /var/tmp/chengli/output/txmud/dist

BUILD SUCCESSFUL
Total time: 0 seconds
Buildfile: /home/chengli/newJava/build.xml

txmud:
    [mkdir] Created dir: /var/tmp/chengli/output/txmud/build
    [javac] /home/chengli/newJava/build.xml:44: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 76 source files to /var/tmp/chengli/output/txmud/build
    [javac] Note: /home/chengli/newJava/src/network/netty/ServerHandler.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.

scratchpad:
    [javac] /home/chengli/newJava/build.xml:106: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 24 source files to /var/tmp/chengli/output/txmud/build
    [javac] Note: /home/chengli/newJava/src/txstore/scratchpad/rdbms/jdbc/TxMudConnection.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.

storageshim:
    [javac] /home/chengli/newJava/build.xml:73: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 55 source files to /var/tmp/chengli/output/txmud/build
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.

fakedstorageshim:
    [javac] /home/chengli/newJava/build.xml:140: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 6 source files to /var/tmp/chengli/output/txmud/build

scratchpadProxy:
    [javac] /home/chengli/newJava/build.xml:169: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds

kvstorage:
    [javac] /home/chengli/newJava/build.xml:154: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds

jars:
    [mkdir] Created dir: /var/tmp/chengli/output/txmud/dist/jars
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/coordinator.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/jdbctxmud.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/storageshim.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/simplestorage.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/simpleproxy.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/openloopuser.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/fakedstorageshim.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/fakedproxy.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/microproxy.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/dbproxy.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/spproxy.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/initDB.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/kvproxy.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/kvstorage.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/kvdirectstorage.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/testproxy.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/updatetable.jar
      [jar] Building jar: /var/tmp/chengli/output/txmud/dist/jars/userSimulator.jar

dist:
  [one-jar] main.jar fs=coordinator.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/coordinator-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=storageshim.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/storageshim-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=simplestorage.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/simplestorage-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=simpleproxy.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/simpleproxy-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=openloopuser.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/openloopuser-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=fakedstorageshim.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/fakedstorageshim-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=fakedproxy.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/fakedproxy-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=microproxy.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/microproxy-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=dbproxy.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/dbproxy-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=spproxy.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/spproxy-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=initDB.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/initDB-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=kvproxy.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/kvproxy-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=kvstorage.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/kvstorage-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=kvdirectstorage.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/kvdirectstorage-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=testproxy.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/testproxy-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=updatetable.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/updatetable-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.
  [one-jar] main.jar fs=userSimulator.jar
  [one-jar] Building jar: /var/tmp/chengli/output/txmud/dist/userSimulator-big.jar
  [one-jar] No 'manifest' attribute was specified for the <one-jar> task, a default manifest will be generated.

BUILD SUCCESSFUL
Total time: 11 seconds
Buildfile: /home/chengli/newJava/src/applications/RUBiStxmud/Client/build.xml

clean:
   [delete] Deleting directory /home/chengli/newJava/src/applications/RUBiStxmud/Client/build

BUILD SUCCESSFUL
Total time: 0 seconds
Buildfile: /home/chengli/newJava/src/applications/RUBiStxmud/Client/build.xml

init:
    [mkdir] Created dir: /home/chengli/newJava/src/applications/RUBiStxmud/Client/build

compile:
    [javac] /home/chengli/newJava/src/applications/RUBiStxmud/Client/build.xml:21: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 16 source files to /home/chengli/newJava/src/applications/RUBiStxmud/Client/build
    [javac] depend attribute is not supported by the modern compiler
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.

dist:
      [jar] Building jar: /home/chengli/newJava/src/applications/RUBiStxmud/Client/rubis_client.jar

BUILD SUCCESSFUL
Total time: 2 seconds

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 1
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 1
total userNum 1

 ===> start create result folders
experiment handler initialization
You want to deploy the experiment since you haven't done yet or code has been changed!

 ===> deploy experiment
initiate datacenters
proxy-0 <thor51.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor53.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor01.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor50.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor48.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 1
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 1
proxy  0 thor51.mpi-sws.org 60003
sshim  0 thor50.mpi-sws.org 60002
database  0 thor56.mpi-sws.org 50000
user  0 thor53.mpi-sws.org 60004
user  0  proxy  (0, 0)
set logical clock
we make DcConfigXml here
create connections
thor48.mpi-sws.org
node  thor48.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor50.mpi-sws.org
node  thor50.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor56.mpi-sws.org
thor51.mpi-sws.org
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor53.mpi-sws.org
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt

 ===> prepare local code and jars
cd /var/tmp/workspace/georeplication/ && ant clean && ant
cd /home/chengli/newJava && ant clean && ant
cd /home/chengli/newJava/src/applications/RUBiStxmud/Client && ant clean && ant

 ===> deploy coordinator and storageshim remotely
copy codes to remote machines
node  thor48.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//coordinator-big.jar
Copying file from source  /var/tmp/workspace/georeplication//dist/exec/coordinator-big.jar  to destination /var/tmp/chengli//coordinator-big.jar
node  thor50.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//storageshim-big.jar
copy jar file  to storage node 0 0
Copying file from source  /var/tmp/workspace/georeplication//dist/exec/storageshim-big.jar  to destination /var/tmp/chengli//storageshim-big.jar

 ===> deploy proxies to remote nodes

 ===> copy files to rubis
Copying file from source  rubis_txmud_db1dc.xml  to destination /home/chengli/newJava/src/applications/RUBiStxmud/Servlets/rubis_txmud_db.xml
Copying file from source  1dc1proxy1storage1user1.xml  to destination /home/chengli/newJava/src/applications/RUBiStxmud/Servlets/rubis_txmud.xml

 ===> deploy website
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; mkdir /var/tmp/chengli/txmud

 ===> compile txmud
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava && ant clean && ant

 ===> Downloading Tomcat6
tomcat directory already exists

 ===> downloading mysql driver
mysql driver already installed

 ===> deploy txmud components
node  thor51.mpi-sws.org
execute cmd  rm -f /var/tmp/chengli/txmud/tomcat6/lib/jsqlparser.jar /var/tmp/chengli/txmud/tomcat6/lib/netty-3.2.1.Final.jar /var/tmp/chengli/txmud/tomcat6/lib/log4j-1.2.15.jar /var/tmp/chengli/txmud/tomcat6/lib/jdbctxmud.jar /var/tmp/chengli/txmud/tomcat6/lib/georeplication.jar
node  thor51.mpi-sws.org
execute cmd  cp /var/tmp/chengli/output/txmud/dist/jars/jdbctxmud.jar /var/tmp/chengli/txmud/tomcat6/lib
node  thor51.mpi-sws.org
execute cmd  cp /var/tmp/workspace/MPI-txmud-java/lib/log4j-1.2.15.jar /var/tmp/chengli/txmud/tomcat6/lib
Copying file from source  /var/tmp/workspace/georeplication//lib/jsqlparser.jar  to destination /var/tmp/chengli/txmud/tomcat6/lib/jsqlparser.jar
Copying file from source  /var/tmp/workspace/georeplication//lib/netty-3.2.1.Final.jar  to destination /var/tmp/chengli/txmud/tomcat6/lib/netty-3.2.1.Final.jar
Copying file from source  /var/tmp/workspace/georeplication//bin/lib/georeplication.jar  to destination /var/tmp/chengli/txmud/tomcat6/lib/georeplication.jar
change proxy to connect to database  thor56.mpi-sws.org 50000
node  thor51.mpi-sws.org
execute cmd  sed -i '/datasource.url/c  \datasource.url    jdbc:mysql://thor56.mpi-sws.org:50000/rubis? '  /home/chengli/newJava/src/applications/RUBiStxmud/Servlets/mysql.properties

 ===> deploy users to remote nodes
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//rubis_client.jar
copy jar file  to client node 0 0
Copying file from source  /home/chengli/newJava/src/applications/RUBiStxmud/Client/rubis_client.jar  to destination /var/tmp/chengli//rubis_client.jar
copy client properties to client node 0 0
Copying file from source  /home/chengli/newJava/src/applications/RUBiStxmud/Client/rubis.properties  to destination /var/tmp/chengli//rubis.properties
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli//bench

 ===> experiment is deployed!
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Deploy is running
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Deploy finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
<?xml version='1.0' encoding='UTF-8'?>
<dataCenters dcNum='1'>
  <dataCenter RemotecdIP='thor48.mpi-sws.org' cdPort='60000' cdIP='thor48.mpi-sws.org' RemotecdPort='60001'>
    <storageShims ssNum='1'>
      <storageShim ssIP='thor50.mpi-sws.org' ssPort='60002'/>
    </storageShims>
    <webProxies wpNum='1'>
      <webproxy wpPort='60003' wpIP='thor51.mpi-sws.org'/>
    </webProxies>
  </dataCenter>
</dataCenters>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 1
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 1
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed your code, now you want to config all of them

 ===> configure experiment
initiate datacenters
proxy-0 <thor51.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor53.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor01.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor50.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor48.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 1
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 1
proxy  0 thor51.mpi-sws.org 60003
sshim  0 thor50.mpi-sws.org 60002
database  0 thor56.mpi-sws.org 50000
user  0 thor53.mpi-sws.org 60004
user  0  proxy  (0, 0)
set logical clock
we make DcConfigXml here
create connections
thor48.mpi-sws.org
node  thor48.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor50.mpi-sws.org
node  thor50.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor56.mpi-sws.org
thor51.mpi-sws.org
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor53.mpi-sws.org
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
copy Files to remote machines
Copying file from source  1dc1proxy1storage1user1.xml  to destination /var/tmp/chengli//1dc1proxy1storage1user1.xml
copy file  to storage node 0 0
Copying file from source  1dc1proxy1storage1user1.xml  to destination /var/tmp/chengli//1dc1proxy1storage1user1.xml
Copying file from source  rubis_txmud_db1dc.xml  to destination /var/tmp/chengli//rubis_txmud_db1dc.xml
install proxy all websites
install website
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant clean undeploy dist deploy -Dbackend=sifter -Dtotalproxy=1 -DdcCount=1 -DdcId=0 -DproxyId=0 -Ddbpool=100
install all user websites
install client
node  thor53.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Client && ant clean && ant
try to configure user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  sed -i '/workload_up_ramp_time_in_ms/c  \workload_up_ramp_time_in_ms  =60000' /var/tmp/chengli//rubis.properties && sed -i '/workload_session_run_time_in_ms/c  \workload_session_run_time_in_ms  =300000' /var/tmp/chengli//rubis.properties && sed -i '/workload_down_ramp_time_in_ms/c  \workload_down_ramp_time_in_ms  =60000' /var/tmp/chengli//rubis.properties && sed -i '/httpd_hostname/c \httpd_hostname =thor51.mpi-sws.org' /var/tmp/chengli//rubis.properties
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Config is running
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Config finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
<?xml version='1.0' encoding='UTF-8'?>
<dataCenters dcNum='1'>
  <dataCenter RemotecdIP='thor48.mpi-sws.org' cdPort='60000' cdIP='thor48.mpi-sws.org' RemotecdPort='60001'>
    <storageShims ssNum='1'>
      <storageShim ssIP='thor50.mpi-sws.org' ssPort='60002'/>
    </storageShims>
    <webProxies wpNum='1'>
      <webproxy wpPort='60003' wpIP='thor51.mpi-sws.org'/>
    </webProxies>
  </dataCenter>
</dataCenters>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 1
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 1
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor51.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor53.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor01.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor50.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor48.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 1
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 1
proxy  0 thor51.mpi-sws.org 60003
sshim  0 thor50.mpi-sws.org 60002
database  0 thor56.mpi-sws.org 50000
user  0 thor53.mpi-sws.org 60004
user  0  proxy  (0, 0)
set logical clock
we make DcConfigXml here
create connections
thor48.mpi-sws.org
node  thor48.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor50.mpi-sws.org
node  thor50.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor56.mpi-sws.org
thor51.mpi-sws.org
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor53.mpi-sws.org
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 05:56:47
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 05:56:47' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc1proxy1storage1user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc1proxy1storage1user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 6:57:55']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/rubis_dcId0_userId0.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc1proxyLocalConfigSifter.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-6-55-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/rubis1dc1proxyLocalConfigSifter.txt
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Run is running
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
<?xml version='1.0' encoding='UTF-8'?>
<dataCenters dcNum='1'>
  <dataCenter RemotecdIP='thor48.mpi-sws.org' cdPort='60000' cdIP='thor48.mpi-sws.org' RemotecdPort='60001'>
    <storageShims ssNum='1'>
      <storageShim ssIP='thor50.mpi-sws.org' ssPort='60002'/>
    </storageShims>
    <webProxies wpNum='1'>
      <webproxy wpPort='60003' wpIP='thor51.mpi-sws.org'/>
    </webProxies>
  </dataCenter>
</dataCenters>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 1
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 1
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor51.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor53.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor01.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor50.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor48.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 1
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 1
proxy  0 thor51.mpi-sws.org 60003
sshim  0 thor50.mpi-sws.org 60002
database  0 thor56.mpi-sws.org 50000
user  0 thor53.mpi-sws.org 60004
user  0  proxy  (0, 0)
set logical clock
we make DcConfigXml here
create connections
thor48.mpi-sws.org
node  thor48.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor50.mpi-sws.org
node  thor50.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor56.mpi-sws.org
thor51.mpi-sws.org
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor53.mpi-sws.org
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 06:07:38
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 06:07:38' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc1proxy1storage1user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc1proxy1storage1user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 7:8:56']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/rubis_dcId0_userId0.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc1proxyLocalConfigSifter.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-6-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/rubis1dc1proxyLocalConfigSifter.txt
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Run is running
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
<?xml version='1.0' encoding='UTF-8'?>
<dataCenters dcNum='1'>
  <dataCenter RemotecdIP='thor48.mpi-sws.org' cdPort='60000' cdIP='thor48.mpi-sws.org' RemotecdPort='60001'>
    <storageShims ssNum='1'>
      <storageShim ssIP='thor50.mpi-sws.org' ssPort='60002'/>
    </storageShims>
    <webProxies wpNum='1'>
      <webproxy wpPort='60003' wpIP='thor51.mpi-sws.org'/>
    </webProxies>
  </dataCenter>
</dataCenters>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 1
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 1
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor51.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor53.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor01.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor50.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor48.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 1
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 1
proxy  0 thor51.mpi-sws.org 60003
sshim  0 thor50.mpi-sws.org 60002
database  0 thor56.mpi-sws.org 50000
user  0 thor53.mpi-sws.org 60004
user  0  proxy  (0, 0)
set logical clock
we make DcConfigXml here
create connections
thor48.mpi-sws.org
node  thor48.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor50.mpi-sws.org
node  thor50.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor56.mpi-sws.org
thor51.mpi-sws.org
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor53.mpi-sws.org
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
R
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 06:18:53
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 06:18:53' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc1proxy1storage1user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc1proxy1storage1user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 7:20:11']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/rubis_dcId0_userId0.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc1proxyLocalConfigSifter.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-17-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/rubis1dc1proxyLocalConfigSifter.txt
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Run is running
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Run finished
/usr/lib/python2.6/dist-packages/Crypto/Util/randpool.py:40: RandomPool_DeprecationWarning: This application uses RandomPool, which is BROKEN in older releases.  See http://www.pycrypto.org/randpool-broken
  RandomPool_DeprecationWarning)
<?xml version='1.0' encoding='UTF-8'?>
<databases dbNum='1'>
	<database dcId='0' dbId='0' dbHost='thor56.mpi-sws.org' dbPort='50000' dbUser='root' dbPwd='101010' 
	dbName='rubis'   tableList='bids,buy_now,categories,comments,items,regions,users'    
	tableLWW='bids,buy_now,categories,comments,items,regions,users'         
	talbeOps=''             
	redTable='bids,categories,comments,regions,users' 
	blueTable='items,buy_now' url_prefix='jdbc:mysql://' > 
	</database>
</databases>
<?xml version='1.0' encoding='UTF-8'?>
<dataCenters dcNum='1'>
  <dataCenter RemotecdIP='thor48.mpi-sws.org' cdPort='60000' cdIP='thor48.mpi-sws.org' RemotecdPort='60001'>
    <storageShims ssNum='1'>
      <storageShim ssIP='thor50.mpi-sws.org' ssPort='60002'/>
    </storageShims>
    <webProxies wpNum='1'>
      <webproxy wpPort='60003' wpIP='thor51.mpi-sws.org'/>
    </webProxies>
  </dataCenter>
</dataCenters>

 ==> parse configure file

 ===>here is the initialization function
datacenters:
coorNum 1
user  0  proxy  (0, 0)
userNumList ['1']
blueToken  10000000
expiredTime  10000000000000
bluequietTime 100000000
totalProxyNum: 1
total sshimNum 1
total userNum 1

 ===> start create result folders
experiment handler initialization
You already deployed and configured your code, now you want to launch experiments, Good luck!
prepare connections
initiate datacenters
proxy-0 <thor51.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
user-0 <thor53.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
zookeeper <thor01.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
sshim-0 <thor50.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
coordinator-0 <thor48.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
database-0 <thor56.mpi-sws.org,chengli,aa,/home/chengli/.ssh/id_rsa>
dcId='0' dbId='0'
sed -i "/dcId='0' dbId='0'/s/dbHost='.*' dbPort/dbHost='thor56.mpi-sws.org' dbPort/g" rubis_txmud_db1dc.xml
1 1
assign user 0
total sshimNum 1
totalProxyNum: 1
total userNum 1
all dc
dc  0
coorNum 1
proxy  0 thor51.mpi-sws.org 60003
sshim  0 thor50.mpi-sws.org 60002
database  0 thor56.mpi-sws.org 50000
user  0 thor53.mpi-sws.org 60004
user  0  proxy  (0, 0)
set logical clock
we make DcConfigXml here
create connections
thor48.mpi-sws.org
node  thor48.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor50.mpi-sws.org
node  thor50.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor56.mpi-sws.org
thor51.mpi-sws.org
node  thor51.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
thor53.mpi-sws.org
node  thor53.mpi-sws.org
execute cmd  mkdir /var/tmp/chengli/; rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE
node  139.19.131.115
execute cmd  rm -rf /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20; mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20
node  139.19.131.115
execute cmd  mkdir /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1

 ===>run test now

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
node  thor48.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor56.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor50.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor53.mpi-sws.org
execute cmd  set enforce 0; iptables -F
node  thor51.mpi-sws.org
execute cmd  set enforce 0; iptables -F
clean files
node  thor48.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor50.mpi-sws.org
execute cmd   rm /var/tmp/chengli//*.log; rm /var/tmp/chengli//*.txt
node  thor51.mpi-sws.org
execute cmd  rm /var/tmp/chengli/txmud/tomcat6/logs/* 
node  thor53.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli//*.log && rm -rf /var/tmp/chengli//bench/rubis_dcId*
clean database here 0 0
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor56.mpi-sws.org
execute cmd  rm -rf /var/tmp/chengli/mysql-5.5.18/data
node  thor56.mpi-sws.org
execute cmd  tar -xvf /var/tmp/chengli/mysql-5.5.18.zero.1dc.lc0-0.rubis.tar -C /var/tmp/chengli/
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18
start mysql server here
clear bin log
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm mysqld-bin*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/data; rm ib_logfile*;
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin;nohup ./mysqld_safe --defaults-file=../mysql-test/include/default_mysqld.cnf --port=50000 --innodb_lock_wait_timeout=1 --max_connections=5000 --innodb_buffer_pool_size=8G --innodb_flush_method=O_DIRECT --skip-innodb_doublewrite --innodb_flush_log_at_trx_commit=0 --innodb_log_file_size=128M --disable-log-bin --innodb_log_buffer_size=8M --open-files-limit=5000 --table_open_cache=5000 --transaction-isolation=REPEATABLE-READ --query_cache_size=512M > mysqld.log &
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
ok you started mysqld
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
D
database is not ready
check database here 0 0
node  thor56.mpi-sws.org
execute cmd  ps -u chengli -o pid,state,command |grep -v PID | grep mysqld | grep -v safe | grep -v grep | awk '{print $2}'
S
database already start up
all databases are started
preload database here 0 0
node  thor56.mpi-sws.org
execute cmd   cd /var/tmp/chengli/ && nohup java -jar preloadDB-big.jar rubis &> preloadDB.log
0 size of dcs  1
checking database is ready or not 0 0
node  thor56.mpi-sws.org
execute cmd  tail -n 1 /var/tmp/chengli//preloadDB.log
Finishing preload all database
utc timestamp we get now is  2014-01-26 06:30:12
refresh database here 0 0
node  thor56.mpi-sws.org
execute cmd  cd /var/tmp/chengli/mysql-5.5.18/bin && ./mysql --defaults-file=../mysql-test/include/default_mysqld.cnf --user=root --password=101010 --port=50000 rubis -e  "update items set end_date  = '2014-01-26 06:30:12' + interval FLOOR(1+(RAND(13)*7)) day where id <= 33000"
Finishing refresh all database
blueTokenNum 10000000
node  thor48.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -jar coordinator-big.jar 1dc1proxy1storage1user1.xml 0 0 20 10000000 true 10000000000000 100000000 0.5 &> coor0.txt &
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up coordinator 0
try to start all storageshims
scratchpad num : 25
node  thor50.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && nohup java -Xms2000m -Xmx4000m -jar storageshim-big.jar  1dc1proxy1storage1user1.xml rubis_txmud_db1dc.xml 0 0 20 true 25 &>sshim0-0.txt &
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up storageshim 0 0
waiting for storageshims 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
waiting for storageshim 
node  thor50.mpi-sws.org
execute cmd  tail -n 10 /var/tmp/chengli//sshim0-0.txt
try to start all proxies
node  thor51.mpi-sws.org
execute cmd  cd /home/chengli/newJava/src/applications/RUBiStxmud/Servlets && ant start
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up webproxy 0 0
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
proxy  0 0  not ready
node  thor51.mpi-sws.org
execute cmd  grep "Initializing database pool" /var/tmp/chengli/txmud/tomcat6/logs/catalina.out | wc -l 
node  thor51.mpi-sws.org
execute cmd  tail -n 2 /var/tmp/chengli/txmud/tomcat6/logs/catalina.out
proxy  0 0  ready
try to start all users
try to start user  0 0
connect to  0 0
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli/ && sed -i '/workload_number_of_clients_per_node/c \workload_number_of_clients_per_node =1' rubis.properties && nohup java -cp "rubis_client.jar:./" edu.rice.rubis.client.ClientEmulator 0 0 &> user0-0.log &
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
set up user 0 0
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 
wait for user to finish
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli | grep java | grep ClientEmulator | grep -v grep 

 ===> finish experiment

 ==> kill all remote process
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor48.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor48.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
Trying to kill all processes by the specified pattern java
node  thor50.mpi-sws.org
execute cmd  kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor50.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
stop mysql server here
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
not killed, try again
node  thor56.mpi-sws.org
execute cmd  kill -9 $(pidof mysqld)
node  thor56.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep mysqld | grep -v grep
already kill mysqld
node  thor56.mpi-sws.org
execute cmd  rm /tmp/mysql.sock
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
node  thor51.mpi-sws.org
execute cmd  kill -9 $(pidof java);
node  thor51.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
killall user  0 0
kill -9 $(ps aux | grep chengli | grep java | grep -v grep | awk '{print $2}')
node  thor53.mpi-sws.org
execute cmd  ps aux | grep chengli  | grep java | grep -v grep
All such processes are killed!
copy coordinator and data writer log
get a list of files from  thor48.mpi-sws.org /var/tmp/chengli/
['coor0.txt']
Downloading file from source  /var/tmp/chengli//coor0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/coor0.txt
get a list of files from  thor50.mpi-sws.org /var/tmp/chengli/
['sshim0-0.txt']
Downloading file from source  /var/tmp/chengli//sshim0-0.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/sshim0-0.txt
copy webproxy file
get a list of files from  thor51.mpi-sws.org /var/tmp/chengli/txmud/tomcat6/logs
['catalina.2014-01-26.log', 'localhost.2014-01-26.log', 'manager.2014-01-26.log', 'host-manager.2014-01-26.log']
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/catalina.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/catalina.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/localhost.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/localhost.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/manager.2014-01-26.log.dcid0.proxyid0
Downloading file from source  /var/tmp/chengli/txmud/tomcat6/logs/host-manager.2014-01-26.log  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/host-manager.2014-01-26.log.dcid0.proxyid0
copy user file
get a list of files from  thor53.mpi-sws.org /var/tmp/chengli//bench
['rubis_dcId0_userId_0_2014-1-26 7:31:30']
node  thor53.mpi-sws.org
execute cmd  cd /var/tmp/chengli//bench && tar -cvf rubis_dcId0_userId0.tar --exclude='*.jar'  --exclude='.properties' ./
Downloading file from source  /var/tmp/chengli//bench/rubis_dcId0_userId0.tar  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/simulateUser1/rubis_dcId0_userId0.tar
finish to copy data and clean systems
already finished the test
Copying file from source  rubis1dc1proxyLocalConfigSifter.txt  to destination /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE/2014-1-26-7-28-rubis-1dc1proxy1storage1user-as20-p10-c20-ss20/rubis1dc1proxyLocalConfigSifter.txt
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Run is running
python rubisSifterExpeRun.py 139.19.131.115 /DPS/TxMud/archive00/SIEVE/DATA/RUNTIME/RUBiS/SIEVE chengli aa rubis_txmud_db1dc.xml rubis rubis1dc1proxyLocalConfigSifter.txt notused  20 10 20 20 nondebug Local nodesRUBiSSifter1dc1Proy.db Run finished
all together
python batchjob161-RUBiS-Original-Local-1dc-2Proxies.py Run  is runing
python batchjob161-RUBiS-Original-Local-1dc-2Proxies.py Run finished
python batchjob161-RUBiS-Original-Local-1dc-2Proxies.py Run  is runing
python batchjob161-RUBiS-Original-Local-1dc-2Proxies.py Run finished
python batchjob168-RUBiS-Sifter-Local-1dc-2Proxies.py Run  is runing
python batchjob168-RUBiS-Sifter-Local-1dc-2Proxies.py Run finished
python batchjob168-RUBiS-Sifter-Local-1dc-2Proxies.py Run  is runing
python batchjob168-RUBiS-Sifter-Local-1dc-2Proxies.py Run finished
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Deploy  is runing
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Deploy finished
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Config  is runing
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Config finished
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Run  is runing
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Run finished
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Run  is runing
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Run finished
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Run  is runing
python batchjob162-RUBiS-Original-Local-1dc-1Proxy-Latency.py Run finished
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Deploy  is runing
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Deploy finished
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Config  is runing
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Config finished
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Run  is runing
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Run finished
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Run  is runing
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Run finished
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Run  is runing
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Run finished
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Run  is runing
python batchjob169-RUBiS-Sifter-Local-1dc-1Proxy-Latency.py Run finished
